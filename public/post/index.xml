<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on matteoruina.com</title>
    <link>http://matteoruina.com/post/</link>
    <description>Recent content in Posts on matteoruina.com</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Matteo Ruina</copyright>
    <lastBuildDate>Thu, 12 May 2016 09:20:18 +0100</lastBuildDate>
    <atom:link href="http://matteoruina.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>AWS performance and troubleshooting handbook</title>
      <link>http://matteoruina.com/2016/05/12/aws-performance/</link>
      <pubDate>Thu, 12 May 2016 09:20:18 +0100</pubDate>
      
      <guid>http://matteoruina.com/2016/05/12/aws-performance/</guid>
      <description>

&lt;h2 id=&#34;aws:6ed8a024fdcb714d98b58012a6339828&#34;&gt;AWS&lt;/h2&gt;

&lt;h3 id=&#34;elb:6ed8a024fdcb714d98b58012a6339828&#34;&gt;ELB&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;latency (seconds): Time elapsed after the request leaves the load balancer until a response is received.&lt;/li&gt;
&lt;li&gt;healthy_host_count (hosts): Number of healthy instances in each AZ.&lt;/li&gt;
&lt;li&gt;httpcode_elb_5xx (responses/second): Number of requests that fail at the load balancer level. Any other values indicates an issue with the ELB itself, e.g. too many inbound requests.&lt;/li&gt;
&lt;li&gt;surge_queue_length (requests/second): measures the length of the queue that holds inbound requests waiting to be dispatched. That queue length should stay near 0 or requests are waiting in the ELB to be dispatched.
&lt;!-- * spillover_count (requests/second): measures the number of requests that had to be rejected by the ELB because the SurgeQueueLength had reach a maximum. --&gt;&lt;/li&gt;
&lt;li&gt;httpcode_backend_5xx (responses/second): Number of HTTP 5XX response codes generated by registered instances.&lt;/li&gt;
&lt;li&gt;httpcode_backend_2xx (responses/second): Number of HTTP 2XX response codes generated by registered instances.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Little&amp;rsquo;s law&lt;/strong&gt;
The long-term average number of customers in a stable system L is equal to the long-term average effective arrival rate, λ, multiplied by the (Palm‑)average time a customer spends in the system, W; or expressed algebraically: L = λW.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;events&amp;quot;: [],
  &amp;quot;requests&amp;quot;: [
    {
      &amp;quot;q&amp;quot;: &amp;quot;aws.elb.latency{name:$ELB_NAME}*sum:aws.elb.request_count{name:$ELB_NAME}&amp;quot;
    }
  ],
  &amp;quot;viz&amp;quot;: &amp;quot;timeseries&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Linux performance and troubleshooting handbook</title>
      <link>http://matteoruina.com/2016/05/12/linux-performance/</link>
      <pubDate>Thu, 12 May 2016 09:20:18 +0100</pubDate>
      
      <guid>http://matteoruina.com/2016/05/12/linux-performance/</guid>
      <description>

&lt;h1 id=&#34;tools:018bfb4d91cc3bbf82cc682161834852&#34;&gt;Tools&lt;/h1&gt;

&lt;h2 id=&#34;atop:018bfb4d91cc3bbf82cc682161834852&#34;&gt;atop&lt;/h2&gt;

&lt;p&gt;View the contents of a file interactively&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;atop -r &amp;quot;${ATOP_FILE}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Show the next sample from the file &lt;code&gt;t&lt;/code&gt;
Show the previous sample from the file &lt;code&gt;T&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;command-line:018bfb4d91cc3bbf82cc682161834852&#34;&gt;Command line&lt;/h2&gt;

&lt;p&gt;Top 10 process by memory usage in Mb&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ps -eo size,pid,user,command --sort -size | awk &#39;{ hr=$1/1024 ; printf(&amp;quot;%13.2f Mb &amp;quot;,hr) } { for ( x=4 ; x&amp;lt;=NF ; x++ ) { printf(&amp;quot;%s &amp;quot;,$x) } print &amp;quot;&amp;quot; }&#39; | head -n 10
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Consul handbook</title>
      <link>http://matteoruina.com/2016/04/21/consul/</link>
      <pubDate>Thu, 21 Apr 2016 09:20:18 +0100</pubDate>
      
      <guid>http://matteoruina.com/2016/04/21/consul/</guid>
      <description>

&lt;h2 id=&#34;security:39a67147ef8937e621563cf0fee73cf1&#34;&gt;Security&lt;/h2&gt;

&lt;h3 id=&#34;acl:39a67147ef8937e621563cf0fee73cf1&#34;&gt;ACL&lt;/h3&gt;

&lt;p&gt;The ACL is Capability-based, relying on tokens to which fine grained rules can be applied. It is very similar to AWS IAM in many ways.&lt;/p&gt;

&lt;p&gt;Every token has an ID, name, type, and rule set. The ID is a randomly generated UUID, making it unfeasible to guess. The name is opaque to Consul and human readable. The type is either &amp;ldquo;client&amp;rdquo; (meaning the token cannot modify ACL rules) or &amp;ldquo;management&amp;rdquo; (meaning the token is allowed to perform all actions).&lt;/p&gt;

&lt;p&gt;Enforcement is always done by the server nodes. All servers must be configured to provide an &lt;code&gt;acl_datacenter&lt;/code&gt; which enables ACL enforcement but also specifies the authoritative datacenter. Consul does not replicate data cross-WAN and instead relies on RPC forwarding to support Multi-Datacenter configurations. However, because requests can be made across datacenter boundaries, ACL tokens must be valid globally. To avoid replication issues, a single datacenter is considered authoritative and stores all the tokens.&lt;/p&gt;

&lt;p&gt;When a request is made to a server in a non-authoritative datacenter server, it must be resolved into the appropriate policy. This is done by reading the token from the authoritative server and caching the result for a configurable &lt;code&gt;acl_ttl&lt;/code&gt;. The implication of caching is that the cache TTL is an upper bound on the staleness of policy that is enforced. It is possible to set a zero TTL, but this has adverse performance impacts, as every request requires refreshing the policy via a cross-datacenter WAN call.&lt;/p&gt;

&lt;p&gt;The Consul ACL system is designed with flexible rules to accommodate for an outage of the &lt;code&gt;acl_datacenter&lt;/code&gt; or networking issues preventing access to it. In this case, it may be impossible for servers in non-authoritative datacenters to resolve tokens. Consul provides a number of configurable &lt;code&gt;acl_down_policy&lt;/code&gt; choices to tune behavior. It is possible to deny or permit all actions or to ignore cache TTLs and enter a fail-safe mode. The default is to ignore cache TTLs for any previously resolved tokens and to deny any uncached tokens.&lt;/p&gt;

&lt;p&gt;ACLs can also act in either a whitelist or blacklist mode depending on the configuration of &lt;code&gt;acl_default_policy&lt;/code&gt;. If the default policy is to deny all actions, then token rules can be set to whitelist specific actions. In the inverse, the allow all default behavior is a blacklist where rules are used to prohibit actions. &lt;strong&gt;By default, Consul will allow all actions.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;gossip-encryption:39a67147ef8937e621563cf0fee73cf1&#34;&gt;Gossip encryption&lt;/h3&gt;

&lt;p&gt;The key can be set via the encrypt parameter: the value of this setting is a configuration file containing the encryption key.&lt;/p&gt;

&lt;p&gt;The key must be 16-bytes, Base64 encoded. As a convenience, Consul provides the &lt;code&gt;consul keygen&lt;/code&gt; commmand to generate a key.&lt;br /&gt;
The key &lt;strong&gt;must&lt;/strong&gt; be used on both servers and clients; this will prevent rogue consul client to join you server cluster.&lt;/p&gt;

&lt;p&gt;The provided key is automatically persisted to the data directory and loaded automatically whenever the agent is restarted. This means that to encrypt Consul&amp;rsquo;s gossip protocol, this option only needs to be provided once on each agent&amp;rsquo;s initial startup sequence. If it is provided after Consul has been initialized with an encryption key, then the provided key is ignored and a warning will be displayed. To enable encryption then, the &lt;code&gt;serf&lt;/code&gt; folder need to be deleted.&lt;/p&gt;

&lt;p&gt;The key is stored in &lt;code&gt;&amp;quot;${CONSUL_DATA_DIR}&amp;quot;/serf/local.keyring&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;tls-encryption:39a67147ef8937e621563cf0fee73cf1&#34;&gt;TLS encryption&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openssl req -x509 -newkey rsa:2048 -days 3650 -nodes -out ca.cert

[ ca ]
default_ca = myca

[ myca ]
unique_subject = no
new_certs_dir = .
certificate = ca.cert
database = certindex
private_key = privkey.pem
serial = serial
default_days = 3650
default_md = sha1
policy = myca_policy
x509_extensions = myca_extensions

[ myca_policy ]
commonName = supplied
stateOrProvinceName = supplied
countryName = supplied
emailAddress = optional
organizationName = supplied
organizationalUnitName = optional

[ myca_extensions ]
basicConstraints = CA:false
subjectKeyIdentifier = hash
authorityKeyIdentifier = keyid:always
keyUsage = digitalSignature,keyEncipherment
extendedKeyUsage = serverAuth,clientAuth
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;troubleshooting:39a67147ef8937e621563cf0fee73cf1&#34;&gt;Troubleshooting&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Join a cluster with the wrong encryption key&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;==&amp;gt; Starting Consul agent...
==&amp;gt; Starting Consul agent RPC...
==&amp;gt; Joining cluster...
==&amp;gt; EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;sources:39a67147ef8937e621563cf0fee73cf1&#34;&gt;Sources&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.consul.io&#34;&gt;https://www.consul.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mauras.ch/securing-consul.html&#34;&gt;https://www.mauras.ch/securing-consul.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hashicorp/consul/issues/1013&#34;&gt;https://github.com/hashicorp/consul/issues/1013&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-secure-consul-with-tls-encryption-on-ubuntu-14-04&#34;&gt;https://www.digitalocean.com/community/tutorials/how-to-secure-consul-with-tls-encryption-on-ubuntu-14-04&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Nomad notebook</title>
      <link>http://matteoruina.com/2016/04/13/nomad/</link>
      <pubDate>Wed, 13 Apr 2016 09:20:18 +0100</pubDate>
      
      <guid>http://matteoruina.com/2016/04/13/nomad/</guid>
      <description>

&lt;h1 id=&#34;glossary:df70cea2caad6cbe9106fef3b6df1a4b&#34;&gt;Glossary&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Job&lt;/strong&gt;: a Job is a specification provided by users that declares a workload for Nomad. A Job is a form of desired state; the user is expressing that the job should be running, but not where it should be run.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Task Group&lt;/strong&gt;: a Task Group is a set of tasks that must be run together. For example, a web server may require that a log shipping co-process is always running as well. A task group is the unit of scheduling, meaning the entire group must run on the same client node and cannot be split.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Task&lt;/strong&gt;: a Task is the smallest unit of work in Nomad. Tasks are executed by drivers, which allow Nomad to be flexible in the types of tasks it supports. Tasks specify their driver, configuration for the driver, constraints, and resources required.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Allocation&lt;/strong&gt;: an Allocation is a mapping between a task group in a job and a client node. A single job may have hundreds or thousands of task groups, meaning an equivalent number of allocations must exist to map the work to client machines.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Client&lt;/strong&gt;: a Client of Nomad is a machine that tasks can be run on. All clients run the Nomad agent. The agent is responsible for registering with the servers, watching for any work to be assigned and executing tasks. The Nomad agent is a long lived process which interfaces with the servers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Server&lt;/strong&gt;: Nomad servers are the brains of the cluster. There is a cluster of servers per region and they manage all jobs and clients, run evaluations, and create task allocations. The servers replicate data between each other and perform leader election to ensure high availability. Servers federate across regions to make Nomad globally aware.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;basic-concepts:df70cea2caad6cbe9106fef3b6df1a4b&#34;&gt;Basic concepts&lt;/h1&gt;

&lt;p&gt;A datacenter represents something like an AWS availability zone, a physical datacenter location, or some other fault domain or boundary.&lt;br /&gt;
Regions may contain multiple datacenters. Servers are assigned to regions and manage all state for the region and make scheduling decisions within that region. Requests that are made between regions are forwarded to the appropriate servers. As an example, you may have a US region with the us-east-1 and us-west-1 datacenters, connected to the EU region with the eu-fr-1 and eu-uk-1 datacenters.&lt;/p&gt;

&lt;p&gt;A client is a very lightweight process that registers the host machine, performs heartbeating, and runs any tasks that are assigned to it by the servers. The agent must be run on every node that is part of the cluster so that the servers can assign work to those machines.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.nomadproject.io/assets/images/nomad-architecture-region-a5b20915.png&#34; alt=&#34;Single Region&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Within each region, we have both clients and servers. Servers are responsible for accepting jobs from users, managing clients, and computing task placements. Each region may have clients from multiple datacenters, allowing a small number of servers to handle very large clusters.&lt;/p&gt;

&lt;p&gt;Regions are fully independent from each other, and do not share jobs, clients, or state. They are loosely-coupled using a gossip protocol, which allows users to submit jobs to any region or query the state of any region transparently. Requests are forwarded to the appropriate server to be processed and the results returned.&lt;/p&gt;

&lt;p&gt;The servers in each datacenter are all part of a single consensus group. This means that they work together to elect a single leader which has extra duties. The leader is responsible for processing all queries and transactions. Nomad is optimistically concurrent, meaning all servers participate in making scheduling decisions in parallel. The leader provides the additional coordination necessary to do this safely and to ensure clients are not oversubscribed.&lt;/p&gt;

&lt;p&gt;Clients are configured to communicate with their regional servers and communicate using remote procedure calls (RPC) to register themselves, send heartbeats for liveness, wait for new allocations, and update the status of allocations. A client registers with the servers to provide the resources available, attributes, and installed drivers. Servers use this information for scheduling decisions and create allocations to assign work to clients.&lt;/p&gt;

&lt;p&gt;Users make use of the Nomad CLI or API to submit jobs to the servers. A job represents a desired state and provides the set of tasks that should be run. The servers are responsible for scheduling the tasks, which is done by finding an optimal placement for each task such that resource utilization is maximized while satisfying all constraints specified by the job. Resource utilization is maximized by bin packing, in which the scheduling tries to make use of all the resources of a machine without exhausting any dimension. Job constraints can be used to ensure an application is running in an appropriate environment. Constraints can be technical requirements based on hardware features such as architecture and availability of GPUs, or software features like operating system and kernel version, or they can be business constraints like ensuring PCI compliant workloads run on appropriate servers.&lt;/p&gt;

&lt;h2 id=&#34;jobs-and-scheduling:df70cea2caad6cbe9106fef3b6df1a4b&#34;&gt;Jobs and Scheduling&lt;/h2&gt;

&lt;p&gt;There are four primary &amp;ldquo;nouns&amp;rdquo; in Nomad; jobs, nodes, allocations, and evaluations. Jobs are submitted by users and represent a desired state. A job is a declarative description of tasks to run which are bounded by constraints and require resources. Tasks can be scheduled on nodes in the cluster running the Nomad client. The mapping of tasks in a job to clients is done using allocations. An allocation is used to declare that a set of tasks in a job should be run on a particular node. Scheduling is the process of determining the appropriate allocations and is done as part of an evaluation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.nomadproject.io/assets/images/nomad-evaluation-flow-7629d361.png&#34; alt=&#34;Evaluation Flow&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The lifecycle of an evaluation beings with an event causing the evaluation to be created. Evaluations are created in the pending state and are enqueued into the evaluation broker. There is a single evaluation broker which runs on the leader server. The evaluation broker is used to manage the queue of pending evaluations, provide priority ordering, and ensure at least once delivery.&lt;/p&gt;

&lt;p&gt;Nomad servers run scheduling workers, defaulting to one per CPU core, which are used to process evaluations. The workers dequeue evaluations from the broker, and then invoke the appropriate scheduler as specified by the job. Nomad ships with a service scheduler that optimizes for long-lived services, a batch scheduler that is used for fast placement of batch jobs, a system scheduler that is used to run jobs on every node, and a core scheduler which is used for internal maintenance. Nomad can be extended to support custom schedulers as well.&lt;/p&gt;

&lt;p&gt;When planning is complete, the scheduler submits the plan to the leader which adds the plan to the plan queue. The plan queue manages pending plans, provides priority ordering, and allows Nomad to handle concurrency races. Multiple schedulers are running in parallel without locking or reservations, making Nomad optimistically concurrent. As a result, schedulers might overlap work on the same node and cause resource over-subscription. The plan queue allows the leader node to protect against this and do partial or complete rejections of a plan.&lt;/p&gt;

&lt;p&gt;As the leader processes plans, it creates allocations when there is no conflict and otherwise informs the scheduler of a failure in the plan result. The plan result provides feedback to the scheduler, allowing it to terminate or explore alternate plans if the previous plan was partially or completely rejected.&lt;/p&gt;

&lt;p&gt;Once the scheduler has finished processing an evaluation, it updates the status of the evaluation and acknowledges delivery with the evaluation broker. This completes the lifecycle of an evaluation. Allocations that were created, modified or deleted as a result will be picked up by client nodes and will begin execution.&lt;/p&gt;

&lt;h1 id=&#34;job:df70cea2caad6cbe9106fef3b6df1a4b&#34;&gt;Job&lt;/h1&gt;

&lt;h2 id=&#34;service-discovery:df70cea2caad6cbe9106fef3b6df1a4b&#34;&gt;Service Discovery&lt;/h2&gt;

&lt;p&gt;Nomad integrates with Consul for service discovery. A service block represents a routable and discoverable service on the network.&lt;/p&gt;

&lt;p&gt;Nomad schedules workloads of various types across a cluster of generic hosts. Because of this, placement is not known in advance and you will need to use service discovery to connect tasks to other services deployed across your cluster. Nomad integrates with Consul to provide service discovery and monitoring.&lt;/p&gt;

&lt;p&gt;There is no need of a service discovery if the job doesn&amp;rsquo;t need to receive any incoming connection.&lt;/p&gt;

&lt;h2 id=&#34;networking:df70cea2caad6cbe9106fef3b6df1a4b&#34;&gt;Networking&lt;/h2&gt;

&lt;p&gt;This only applies to services that want to listen on a port. Batch jobs or services that only make outbound connections do not need to allocate ports, since they will use any available interface to make an outbound connection.&lt;/p&gt;

&lt;h1 id=&#34;snippets:df70cea2caad6cbe9106fef3b6df1a4b&#34;&gt;Snippets&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Read stdout and stderr&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nomad fs cat &amp;quot;${ALLOC_ID}&amp;quot; alloc/logs/&amp;quot;${TASK_NAME}&amp;quot;.stderr.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Manage jobs&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Start a job
nomad run -verbose test_nomad
nomad status &amp;quot;${JOB_NAME}&amp;quot;
# Pick an ID in the Allocations output, match it with the Node ID
# On the Node
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;links:df70cea2caad6cbe9106fef3b6df1a4b&#34;&gt;Links&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nomadproject.io&#34;&gt;https://www.nomadproject.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hashicorp/nomad/issues/304&#34;&gt;https://github.com/hashicorp/nomad/issues/304&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>